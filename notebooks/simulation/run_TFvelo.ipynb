{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87443fd1",
   "metadata": {},
   "source": [
    "# Calculate velocity and latent time using TFvelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67153cd",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2abba759-163c-42c8-b826-7b91623a9fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import TFvelo as TFv\n",
    "from paths import DATA_DIR, FIG_DIR\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "\n",
    "# attach your local TFvelo repo directory\n",
    "sys.path.append(\"/home/itg/z.xue/VeloBenchmark/TFvelo\")\n",
    "\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0913f35",
   "metadata": {},
   "source": [
    "## General settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fc0858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "SAVE_FIGURES = True\n",
    "if SAVE_FIGURES:\n",
    "    os.makedirs(FIG_DIR / \"simulation\", exist_ok=True)\n",
    "\n",
    "SAVE_DATASETS = True\n",
    "if SAVE_DATASETS:\n",
    "    os.makedirs(DATA_DIR / \"simulation\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c7e90c3-d846-4175-8289-2c2f5336397a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "input_path = DATA_DIR\n",
    "output_path = DATA_DIR / \"simulation\"\n",
    "input_files = os.listdir(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9679dc3a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7789a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_TFvelo(input_path, output_path, input_file):\n",
    "    \"\"\"TODO.\"\"\"\n",
    "    adata = ad.read(os.path.join(input_path, input_file))\n",
    "    print(\"Start processing \" + os.path.join(input_path, input_file))\n",
    "    adata.layers[\"spliced\"] = adata.layers[\"counts_spliced\"].copy()\n",
    "    adata.layers[\"unspliced\"] = adata.layers[\"counts_unspliced\"].copy()\n",
    "\n",
    "    if \"spliced\" in adata.layers:\n",
    "        adata.layers[\"total\"] = adata.layers[\"spliced\"] + adata.layers[\"unspliced\"]\n",
    "    elif \"new\" in adata.layers:\n",
    "        adata.layers[\"total\"] = np.array(adata.layers[\"total\"].todense())\n",
    "    else:\n",
    "        adata.layers[\"total\"] = adata.X\n",
    "    adata.layers[\"count\"] = adata.X.copy()\n",
    "    adata.layers[\"total_raw\"] = adata.layers[\"total\"].copy()\n",
    "    n_cells, n_genes = adata.X.shape\n",
    "    sc.pp.filter_genes(adata, min_cells=int(n_cells / 50))\n",
    "    sc.pp.filter_cells(adata, min_genes=int(n_genes / 50))\n",
    "    TFv.pp.filter_and_normalize(adata, min_shared_counts=20, n_top_genes=2000, log=True)  # include the following steps\n",
    "    adata.X = adata.layers[\"total\"].copy()\n",
    "\n",
    "    gene_names = []\n",
    "    for tmp in adata.var_names:\n",
    "        gene_names.append(tmp.upper())\n",
    "    adata.var_names = gene_names\n",
    "    adata.var_names_make_unique()\n",
    "    adata.obs_names_make_unique()\n",
    "\n",
    "    TFv.pp.moments(adata, n_pcs=30, n_neighbors=30)\n",
    "    adata.X = adata.X.A\n",
    "    n_gene = adata.shape[1]\n",
    "    adata.varm[\"TFs\"] = np.full([n_gene, n_gene], \"blank\")\n",
    "    adata.varm[\"TFs\"] = adata.varm[\"TFs\"].astype(\"U10\")\n",
    "\n",
    "    adata.varm[\"TFs_id\"] = np.full([n_gene, n_gene], -1)\n",
    "    adata.varm[\"TFs_times\"] = np.full([n_gene, n_gene], 0)\n",
    "    adata.varm[\"TFs_correlation\"] = np.full([n_gene, n_gene], 0.0)\n",
    "    adata.varm[\"knockTF_Log2FC\"] = np.full([n_gene, n_gene], 0.0)\n",
    "    adata.var[\"n_TFs\"] = np.zeros(n_gene, dtype=int)\n",
    "\n",
    "    gene_names = adata.var_names.tolist()  # all genes as targets\n",
    "    all_TFs = list(adata.var_names[adata.var[\"is_tf\"]])  # select TFs\n",
    "\n",
    "    for TF_name in all_TFs:\n",
    "        TF_idx = gene_names.index(TF_name)\n",
    "        TF_expression = np.ravel(adata[:, TF_name].X)\n",
    "\n",
    "        for target in gene_names:\n",
    "            target_idx = gene_names.index(target)\n",
    "            if target == TF_name:\n",
    "                continue\n",
    "\n",
    "            if TF_name in adata.varm[\"TFs\"][target_idx]:\n",
    "                ii = list(adata.varm[\"TFs\"][target_idx]).index(TF_name)\n",
    "                adata.varm[\"TFs_times\"][target_idx, ii] += 1\n",
    "                continue\n",
    "            target_expression = np.ravel(adata[:, target].X)\n",
    "            flag = (TF_expression > 0) & (target_expression > 0)  # consider all possible regulation\n",
    "            if flag.sum() < 2:\n",
    "                correlation = 0\n",
    "            else:\n",
    "                correlation, _ = scipy.stats.spearmanr(target_expression[flag], TF_expression[flag])\n",
    "\n",
    "            tmp_n_TF = adata.var[\"n_TFs\"][target_idx]\n",
    "            adata.varm[\"TFs\"][target_idx][tmp_n_TF] = TF_name\n",
    "            adata.varm[\"TFs_id\"][target_idx][tmp_n_TF] = TF_idx\n",
    "            adata.varm[\"TFs_times\"][target_idx, tmp_n_TF] = 1\n",
    "            adata.varm[\"TFs_correlation\"][target_idx, tmp_n_TF] = correlation\n",
    "            adata.var[\"n_TFs\"][target_idx] += 1\n",
    "    TFv.tl.recover_dynamics(\n",
    "        adata,\n",
    "        n_jobs=64,\n",
    "        max_iter=20,\n",
    "        var_names=\"all\",\n",
    "        WX_method=\"lsq_linear\",\n",
    "        WX_thres=20,\n",
    "        n_top_genes=adata.shape[1],\n",
    "        fit_scaling=True,\n",
    "        use_raw=0,\n",
    "        init_weight_method=\"ones\",\n",
    "        n_time_points=1000,\n",
    "    )\n",
    "    n_cells = adata.shape[0]\n",
    "    expanded_scaling_y = np.expand_dims(np.array(adata.var[\"fit_scaling_y\"]), 0).repeat(n_cells, axis=0)\n",
    "    adata.layers[\"velocity\"] = adata.layers[\"velo_hat\"] / expanded_scaling_y\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949e1b43",
   "metadata": {},
   "source": [
    "## Data loading and processing of one instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07cd49dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing /home/itg/z.xue/VeloBenchmark/50_time_simulations/dataset_sim41.h5ad\n",
      "Normalized count data: X, spliced, unspliced, total.\n",
      "Skip filtering by dispersion since number of variables are less than `n_top_genes`.\n",
      "Logarithmized X.\n",
      "computing neighbors\n",
      "    finished (0:00:12) --> added \n",
      "    'distances' and 'connectivities', weighted adjacency matrices (adata.obsp)\n",
      "computing moments based on connectivities\n",
      "    finished (0:00:00) --> added \n",
      "    'M_total', moments of total abundances (adata.layers)\n",
      "recovering dynamics (using 48/48 cores)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3dc54128e3941f9be3e2edfe0613826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?gene/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0/2 A2_TF1\n",
      "Processing 0/2 A1_TF1\n",
      "Processing 0/2 A4_TF2\n",
      "Processing 0/2 BURN1_TF1\n",
      "Processing 0/2 A3_TF1\n",
      "Processing 0/2 BURN2_TF1\n",
      "Processing 0/2 B3_TF2\n",
      "Processing 0/2 BURN4_TF1\n",
      "Processing 0/2 A3_TF3\n",
      "Processing 0/2 B5_TF1\n",
      "Processing 0/2 B1_TF2\n",
      "Processing 0/2 BURN3_TF1\n",
      "Processing 0/2 B2_TF2\n",
      "Processing 0/2 B4_TF2\n",
      "Processing 0/2 B7_TF1\n",
      "WARNING: 0/2 B9_TF1 not recoverable due to insufficient samples.\n",
      "WARNING: 0/2 B11_TF1 not recoverable due to insufficient samples.\n",
      "WARNING: 1/2 B9_TF2 not recoverable due to insufficient samples.\n",
      "WARNING: 1/2 B11_TF2 not recoverable due to insufficient samples.\n",
      "Processing 0/2 B6_TF1\n",
      "WARNING: 0/2 B12_TF1 not recoverable due to insufficient samples.\n",
      "WARNING: 1/2 B12_TF2 not recoverable due to insufficient samples.\n",
      "WARNING: 0/2 B13_TF2 not recoverable due to insufficient samples.\n",
      "Processing 0/2 B8_TF1\n",
      "WARNING: 1/2 B14_TF1 not recoverable due to insufficient samples.\n",
      "WARNING: 0/2 B10_TF1 not recoverable due to insufficient samples.\n",
      "WARNING: 1/2 B10_TF2 not recoverable due to insufficient samples.\n",
      "WARNING: 0/2 C1_TF2 not recoverable due to insufficient samples.\n",
      "WARNING: 1/2 C2_TF1 not recoverable due to insufficient samples.\n",
      "Processing 0/2 A5_TF2\n",
      "WARNING: 0/2 B14_TF2 not recoverable due to insufficient samples.\n",
      "WARNING: 1/2 C1_TF1 not recoverable due to insufficient samples.\n",
      "WARNING: 0/2 B12_TF3 not recoverable due to insufficient samples.\n",
      "WARNING: 1/2 B13_TF1 not recoverable due to insufficient samples.\n",
      "WARNING: 0/2 C3_TF2 not recoverable due to insufficient samples.\n",
      "WARNING: 0/2 C6_TF1 not recoverable due to insufficient samples.\n",
      "WARNING: 1/2 C4_TF1 not recoverable due to insufficient samples.\n",
      "WARNING: 1/2 C6_TF2 not recoverable due to insufficient samples.\n",
      "WARNING: 0/2 C4_TF2 not recoverable due to insufficient samples.\n",
      "WARNING: 1/2 C5_TF1 not recoverable due to insufficient samples.\n",
      "WARNING: 0/2 C2_TF2 not recoverable due to insufficient samples.\n",
      "WARNING: 1/2 C3_TF1 not recoverable due to insufficient samples.\n",
      "WARNING: 0/2 D3_TF1 not recoverable due to insufficient samples.\n",
      "WARNING: 1/2 D3_TF2 not recoverable due to insufficient samples.\n",
      "WARNING: 0/2 D2_TF1 not recoverable due to insufficient samples.\n",
      "WARNING: 1/2 D2_TF2 not recoverable due to insufficient samples.\n",
      "WARNING: 0/2 C5_TF2 not recoverable due to insufficient samples.\n",
      "WARNING: 1/2 C5_TF3 not recoverable due to insufficient samples.\n",
      "WARNING: 0/2 D4_TF1 not recoverable due to insufficient samples.\n",
      "WARNING: 1/2 D4_TF2 not recoverable due to insufficient samples.\n",
      "Processing 0/2 HK1\n",
      "WARNING: 0/2 D1_TF1 not recoverable due to insufficient samples.\n",
      "WARNING: 1/2 D1_TF2 not recoverable due to insufficient samples.\n",
      "Processing 0/2 TARGET3\n",
      "Processing 0/2 TARGET9\n",
      "Processing 0/1 HK8\n",
      "Processing 0/2 HK3\n",
      "WARNING: 0/2 TARGET5 not recoverable due to insufficient samples.\n",
      "WARNING: 0/2 TARGET7 not recoverable due to insufficient samples.\n",
      "WARNING: 1/2 TARGET6 not recoverable due to insufficient samples.\n",
      "Processing 0/1 HK5\n",
      "Processing 0/1 HK6\n",
      "Processing 1/2 TARGET8\n",
      "Processing 0/2 TARGET1\n",
      "Processing 0/1 HK10\n",
      "Processing 0/1 HK9\n",
      "Processing 0/1 HK7\n",
      "0/2 B6_TF1 FINISHED with n_TFs: 69\n",
      "Processing 1/2 B6_TF2\n",
      "0/2 B8_TF1 FINISHED with n_TFs: 69\n",
      "Processing 1/2 B8_TF2\n",
      "0/2 A3_TF1 FINISHED with n_TFs: 69\n",
      "Processing 1/2 A3_TF2\n",
      "0/2 B5_TF1 FINISHED with n_TFs: 69\n",
      "Processing 1/2 B5_TF2\n",
      "0/2 BURN2_TF1 FINISHED with n_TFs: 69\n",
      "Processing 1/2 BURN2_TF2\n",
      "0/2 A2_TF1 FINISHED with n_TFs: 69\n",
      "Processing 1/2 A2_TF2\n",
      "0/2 A3_TF3 FINISHED with n_TFs: 69\n",
      "Processing 1/2 A4_TF1\n",
      "0/1 HK7 FINISHED with n_TFs: 70\n",
      "0/2 A5_TF2 FINISHED with n_TFs: 69\n",
      "Processing 1/2 B1_TF1\n",
      "1/2 B6_TF2 FINISHED with n_TFs: 69\n",
      "0/2 B7_TF1 FINISHED with n_TFs: 69\n",
      "Processing 1/2 B7_TF2\n",
      "0/2 B2_TF2 FINISHED with n_TFs: 69\n",
      "Processing 1/2 B3_TF1\n",
      "1/2 TARGET8 FINISHED with n_TFs: 70\n",
      "0/1 HK6 FINISHED with n_TFs: 70\n",
      "0/2 B1_TF2 FINISHED with n_TFs: 69\n",
      "Processing 1/2 B2_TF1\n",
      "0/1 HK9 FINISHED with n_TFs: 70\n",
      "1/2 B8_TF2 FINISHED with n_TFs: 69\n",
      "0/2 B4_TF2 FINISHED with n_TFs: 69\n",
      "Processing 1/2 B4_TF3\n",
      "0/2 B3_TF2 FINISHED with n_TFs: 69\n",
      "Processing 1/2 B4_TF1\n",
      "0/2 HK3 FINISHED with n_TFs: 70\n",
      "Processing 1/2 HK4\n",
      "0/2 A4_TF2 FINISHED with n_TFs: 69\n",
      "Processing 1/2 A5_TF1\n",
      "0/2 BURN1_TF1 FINISHED with n_TFs: 69\n",
      "Processing 1/2 BURN1_TF2\n",
      "0/2 TARGET3 FINISHED with n_TFs: 70\n",
      "WARNING: 1/2 TARGET4 not recoverable due to insufficient samples.\n",
      "1/2 A2_TF2 FINISHED with n_TFs: 69\n",
      "0/2 TARGET9 FINISHED with n_TFs: 70\n",
      "Processing 1/2 TARGET10\n",
      "0/2 BURN3_TF1 FINISHED with n_TFs: 69\n",
      "Processing 1/2 BURN3_TF2\n",
      "0/1 HK8 FINISHED with n_TFs: 70\n",
      "0/2 HK1 FINISHED with n_TFs: 70\n",
      "Processing 1/2 HK2\n",
      "0/1 HK5 FINISHED with n_TFs: 70\n",
      "1/2 B5_TF2 FINISHED with n_TFs: 69\n",
      "0/2 TARGET1 FINISHED with n_TFs: 70\n",
      "Processing 1/2 TARGET2\n",
      "1/2 B7_TF2 FINISHED with n_TFs: 69\n",
      "1/2 B2_TF1 FINISHED with n_TFs: 69\n",
      "1/2 BURN2_TF2 FINISHED with n_TFs: 69\n",
      "1/2 A3_TF2 FINISHED with n_TFs: 69\n",
      "0/2 BURN4_TF1 FINISHED with n_TFs: 69\n",
      "Processing 1/2 BURN4_TF2\n",
      "1/2 A4_TF1 FINISHED with n_TFs: 69\n",
      "0/2 A1_TF1 FINISHED with n_TFs: 69\n",
      "Processing 1/2 A1_TF2\n",
      "1/2 B3_TF1 FINISHED with n_TFs: 69\n",
      "1/2 A5_TF1 FINISHED with n_TFs: 69\n",
      "1/2 BURN1_TF2 FINISHED with n_TFs: 69\n",
      "1/2 B1_TF1 FINISHED with n_TFs: 69\n",
      "0/1 HK10 FINISHED with n_TFs: 70\n",
      "1/2 B4_TF3 FINISHED with n_TFs: 69\n",
      "1/2 TARGET10 FINISHED with n_TFs: 70\n",
      "1/2 HK4 FINISHED with n_TFs: 70\n",
      "1/2 BURN3_TF2 FINISHED with n_TFs: 69\n",
      "1/2 HK2 FINISHED with n_TFs: 70\n",
      "1/2 B4_TF1 FINISHED with n_TFs: 69\n",
      "1/2 BURN4_TF2 FINISHED with n_TFs: 69\n",
      "1/2 A1_TF2 FINISHED with n_TFs: 69\n",
      "1/2 TARGET2 FINISHED with n_TFs: 70\n",
      "    finished (0:00:47) --> added \n",
      "    'fit_pars', fitted parameters for splicing dynamics (adata.var)\n"
     ]
    }
   ],
   "source": [
    "adata = run_TFvelo(input_path, output_path, input_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41269bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 300 × 90\n",
       "    obs: 'step_ix', 'simulation_i', 'sim_time', 'n_genes', 'initial_size_spliced', 'initial_size_unspliced', 'initial_size_total', 'initial_size', 'n_counts'\n",
       "    var: 'module_id', 'basal', 'burn', 'independence', 'color', 'is_tf', 'is_hk', 'transcription_rate', 'splicing_rate', 'translation_rate', 'mrna_halflife', 'protein_halflife', 'mrna_decay_rate', 'protein_decay_rate', 'max_premrna', 'max_mrna', 'max_protein', 'mol_premrna', 'mol_mrna', 'mol_protein', 'n_cells', 'n_TFs', 'fit_alpha', 'fit_beta', 'fit_omega', 'fit_theta', 'fit_gamma', 'fit_delta', 'fit_likelihood', 'fit_varx', 'fit_scaling_y'\n",
       "    uns: 'network', 'regulators', 'regulatory_network', 'regulatory_network_regulators', 'regulatory_network_targets', 'skeleton', 'targets', 'traj_dimred_segments', 'traj_milestone_network', 'traj_progressions', 'pca', 'neighbors', 'recover_dynamics'\n",
       "    obsm: 'dimred', 'regulatory_network_sc', 'X_pca'\n",
       "    varm: 'PCs', 'TFs', 'TFs_id', 'TFs_times', 'TFs_correlation', 'knockTF_Log2FC', 'fit_scaling', 'fit_weights', 'fit_weights_init', 'fit_weights_final', 'loss'\n",
       "    layers: 'counts_protein', 'counts_spliced', 'counts_unspliced', 'logcounts', 'rna_velocity', 'spliced', 'unspliced', 'total', 'count', 'total_raw', 'M_total', 'velo_hat', 'velo_t', 'y_t', 'velo_normed', 'filtered', 'WX', 'y', 'fit_t_raw', 'fit_t', 'velocity'\n",
       "    obsp: 'distances', 'connectivities'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit_t and velocity are the computed results\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "166bfc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "if SAVE_DATASETS:\n",
    "    adata.write_h5ad(DATA_DIR / \"simulation\" / \"c2f_output.h5ad\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
