{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b5683f0-b585-4173-9712-fc80bb850243",
   "metadata": {},
   "source": [
    "# Simulate Toy GRN to benchmark velocity, latent time and GRN inference performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7757dfc2-36bb-4d99-aa2e-a503fe9c7a87",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6cc7442-1278-45c4-8281-4c8fe1bff713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import torch\n",
    "import torchsde\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import celloracle as co\n",
    "import scanpy as sc\n",
    "import scvelo as scv\n",
    "from anndata import AnnData\n",
    "from arboreto.algo import grnboost2\n",
    "from regvelo import REGVELOVI\n",
    "from velovi import preprocess_data, VELOVI\n",
    "\n",
    "from rgv_tools import DATA_DIR, FIG_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a05faa-564b-4250-af7f-69322211d4a9",
   "metadata": {},
   "source": [
    "## General settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_defaults()\n",
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee5dd86-3eba-4422-965b-7fc09c0aa62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scv.settings.set_figure_params(\"scvelo\", dpi_save=400, dpi=80, transparent=True, fontsize=20, color_map=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d37b365-2527-4278-ac25-cd9e000a8138",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "SAVE_FIGURES = True\n",
    "if SAVE_FIGURES:\n",
    "    (FIG_DIR / \"simulation\" / \"toy_GRN\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SAVE_DATASETS = True\n",
    "if SAVE_DATASETS:\n",
    "    (DATA_DIR / \"simulation\" / \"toy_GRN\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f4dd6f-3ec3-463f-af98-33d44031bc95",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Function defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df8a32bc-1984-4d77-9ea1-bf0ea0cb45eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_concordance(GRN, ref_GRN):\n",
    "    \"\"\"TODO.\"\"\"\n",
    "    sign_GRN = np.sign(GRN)[GRN != 0]\n",
    "    sign_ref_GRN = np.sign(ref_GRN)[GRN != 0]\n",
    "    score = sum(sign_GRN == sign_ref_GRN)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc171ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_power_matrix(A, B):\n",
    "    \"\"\"TODO.\"\"\"\n",
    "    if len(A) != len(B) or len(A[0]) != len(B[0]):\n",
    "        raise ValueError(\"Both matrices must have the same dimensions\")\n",
    "\n",
    "    rows = len(A)\n",
    "    cols = len(A[0])\n",
    "\n",
    "    C = [[0 for _ in range(cols)] for _ in range(rows)]\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            C[i][j] = A[i][j] ** B[i][j]\n",
    "\n",
    "    return np.array(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ab166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_poisson(n, random_seed):\n",
    "    \"\"\"TODO.\"\"\"\n",
    "    from random import seed, uniform  # draw from poisson\n",
    "\n",
    "    seed(random_seed)\n",
    "    t = np.cumsum([-0.1 * np.log(uniform(0, 1)) for _ in range(n - 1)])\n",
    "    return np.insert(t, 0, 0)  # prepend t0=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class velocity_encoder(torch.nn.Module):\n",
    "    \"\"\"TODO.\"\"\"\n",
    "\n",
    "    noise_type = \"scalar\"\n",
    "    sde_type = \"ito\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        K,\n",
    "        n,\n",
    "        h,\n",
    "        alpha_b,\n",
    "        beta,\n",
    "        gamma,\n",
    "    ):\n",
    "        \"\"\"TODO.\"\"\"\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.n = n\n",
    "        self.h = h\n",
    "        self.alpha_b = alpha_b\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "    # Drift\n",
    "    def f(self, t, y):\n",
    "        \"\"\"TODO.\"\"\"\n",
    "        y = y.T\n",
    "        u = y[0 : int(y.shape[0] / 2), 0].ravel()\n",
    "        s = y[int(y.shape[0] / 2) :, 0].ravel()\n",
    "\n",
    "        sign = torch.sign(self.K)\n",
    "        sign = torch.clip(torch.sign(self.K), 0, 1)\n",
    "\n",
    "        s_m = s.repeat(self.n.shape[0], 1)\n",
    "        x_n = torch.pow(\n",
    "            torch.clip(\n",
    "                s_m,\n",
    "                0,\n",
    "            ),\n",
    "            self.n,\n",
    "        )\n",
    "        h_n = self.h**self.n\n",
    "\n",
    "        p_act = x_n / (h_n + x_n)\n",
    "        p_neg = h_n / (h_n + x_n)\n",
    "\n",
    "        p = torch.abs(self.K) * ((p_act * sign) + (p_neg * (1 - sign)))\n",
    "        alpha = p.sum(1) + self.alpha_b\n",
    "\n",
    "        du = (\n",
    "            torch.clip(\n",
    "                alpha,\n",
    "                0,\n",
    "            )\n",
    "            - self.beta * u\n",
    "        )\n",
    "        ds = self.beta * u - self.gamma * s\n",
    "\n",
    "        du = du.reshape((-1, 1))\n",
    "        ds = ds.reshape((-1, 1))\n",
    "\n",
    "        v = torch.concatenate([du, ds]).reshape(1, -1)\n",
    "\n",
    "        return v\n",
    "\n",
    "    # Diffusion\n",
    "    def g(self, t, y):\n",
    "        \"\"\"TODO.\"\"\"\n",
    "        return 0.1 * torch.randn([1, y.shape[1]]).view(1, y.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "961fd423-c519-43bd-bc9b-fa344b942319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_regvelo_outputs_to_adata(adata_raw, vae, filter=False):\n",
    "    \"\"\"TODO.\"\"\"\n",
    "    latent_time = vae.get_latent_time(n_samples=30, batch_size=adata_raw.shape[0])\n",
    "    velocities = vae.get_velocity(n_samples=30, batch_size=adata_raw.shape[0])\n",
    "\n",
    "    t = latent_time\n",
    "    scaling = 20 / t.max(0)\n",
    "    adata = adata_raw[:, vae.module.target_index].copy()\n",
    "\n",
    "    adata.layers[\"velocity\"] = velocities\n",
    "    adata.layers[\"latent_time_regvelo\"] = latent_time\n",
    "\n",
    "    adata.layers[\"fit_t\"] = latent_time.values * np.array(scaling)[np.newaxis, :]\n",
    "    adata.var[\"fit_scaling\"] = 1.0\n",
    "    adata.var[\"fit_beta_regvelo\"] = (\n",
    "        torch.clip(torch.nn.functional.softplus(vae.module.v_encoder.beta_mean_unconstr), 0, 50).cpu().detach().numpy()\n",
    "        / scaling\n",
    "    )\n",
    "    adata.var[\"fit_gamma_regvelo\"] = (\n",
    "        torch.clip(torch.nn.functional.softplus(vae.module.v_encoder.gamma_mean_unconstr), 0, 50).cpu().detach().numpy()\n",
    "        / scaling\n",
    "    )\n",
    "\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a140cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_velovi_outputs_to_adata(adata, vae):\n",
    "    \"\"\"TODO.\"\"\"\n",
    "    latent_time = vae.get_latent_time(n_samples=30)\n",
    "    velocities = vae.get_velocity(n_samples=30)\n",
    "\n",
    "    t = latent_time\n",
    "    scaling = 20 / t.max(0)\n",
    "\n",
    "    adata.layers[\"velocity\"] = velocities / scaling\n",
    "    adata.layers[\"latent_time_velovi\"] = latent_time\n",
    "\n",
    "    adata.var[\"fit_alpha_velovi\"] = vae.get_rates()[\"alpha\"] / scaling\n",
    "    adata.var[\"fit_beta_velovi\"] = vae.get_rates()[\"beta\"] / scaling\n",
    "    adata.var[\"fit_gamma_velovi\"] = vae.get_rates()[\"gamma\"] / scaling\n",
    "    adata.var[\"fit_t_\"] = (\n",
    "        torch.nn.functional.softplus(vae.module.switch_time_unconstr).detach().cpu().numpy()\n",
    "    ) * scaling\n",
    "    adata.layers[\"fit_t\"] = latent_time.values * np.array(scaling)[np.newaxis, :]\n",
    "    adata.var[\"fit_scaling\"] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47004461-448a-4759-8e7c-72c085ac9870",
   "metadata": {},
   "source": [
    "## Initialize all values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee5d7dc8-ab1e-4199-9a2f-16c788d7d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "velo_scv = []\n",
    "velo_velovi = []\n",
    "velo_rgv = []\n",
    "corr_t_rgv = []\n",
    "corr_t_scv = []\n",
    "corr_t_velovi = []\n",
    "corr_t_emperical = []\n",
    "corr_t_dpt = []\n",
    "beta_list = []\n",
    "gamma_list = []\n",
    "rgv_GRN = []\n",
    "cor_GRN = []\n",
    "gb_GRN = []\n",
    "co_GRN = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199d1e64-659e-4f40-b7c5-e4a88d34aad4",
   "metadata": {},
   "source": [
    "## Run simulation for 100 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc3777-5af4-4519-9ba8-bebef769ecc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sim_idx in range(100):\n",
    "    print(sim_idx)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.manual_seed(sim_idx)\n",
    "\n",
    "    ## simulate alpha beta and gamma\n",
    "    n_vars = 6\n",
    "\n",
    "    mu = np.array([np.log(5), np.log(0.5), np.log(0.125)])\n",
    "\n",
    "    R = np.array([[1.0, 0.2, 0.2], [0.2, 1.0, 0.8], [0.2, 0.8, 1.0]])\n",
    "\n",
    "    C = np.array([0.4, 0.4, 0.4])[:, None]\n",
    "\n",
    "    cov = C.dot(C.T) * R\n",
    "\n",
    "    alpha, beta, gamma = np.exp(np.random.multivariate_normal(mu, cov, size=n_vars).T)  # multivariate log-normal\n",
    "    # beta /= 3\n",
    "    # gamma /= 10\n",
    "    beta_list.append(beta)\n",
    "    gamma_list.append(gamma)\n",
    "\n",
    "    ##\n",
    "    n_regulators = n_targets = 6\n",
    "    coef_m = np.array(\n",
    "        [\n",
    "            [0, 1, -alpha.mean(), 2, 2],\n",
    "            [1, 0, -alpha.mean(), 2, 2],\n",
    "            [0, 2, alpha.mean(), 2, 4],\n",
    "            [0, 3, alpha.mean(), 2, 4],\n",
    "            [2, 3, -alpha.mean(), 2, 2],\n",
    "            [3, 2, -alpha.mean(), 2, 2],\n",
    "            [1, 4, alpha.mean(), 2, 4],\n",
    "            [1, 5, alpha.mean(), 2, 4],\n",
    "            [4, 5, -alpha.mean(), 2, 2],\n",
    "            [5, 4, -alpha.mean(), 2, 2],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    random_seed = sim_idx\n",
    "\n",
    "    t = torch.tensor(draw_poisson(1500), random_seed=sim_idx)\n",
    "\n",
    "    K = np.zeros([n_targets, n_regulators])\n",
    "    n = np.zeros([n_targets, n_regulators])\n",
    "    h = np.zeros([n_targets, n_regulators])\n",
    "\n",
    "    K[np.array(coef_m[:, 1], dtype=int), np.array(coef_m[:, 0], dtype=int)] = coef_m[:, 2]\n",
    "    n[np.array(coef_m[:, 1], dtype=int), np.array(coef_m[:, 0], dtype=int)] = coef_m[:, 3]\n",
    "    h[np.array(coef_m[:, 1], dtype=int), np.array(coef_m[:, 0], dtype=int)] = coef_m[:, 4]\n",
    "\n",
    "    sde = velocity_encoder(\n",
    "        K=torch.tensor(K, dtype=torch.float32),\n",
    "        n=torch.tensor(n, dtype=torch.float32),\n",
    "        h=torch.tensor(h, dtype=torch.float32),\n",
    "        alpha_b=torch.tensor([1, 1, 1, 1, 1, 1], dtype=torch.float32) * 0,\n",
    "        beta=torch.tensor(beta, dtype=torch.float32),\n",
    "        gamma=torch.tensor(gamma, dtype=torch.float32),\n",
    "    )\n",
    "\n",
    "    ## set up G batches, Each G represent a module (a target gene centerred regulon)\n",
    "    ## infer the observe gene expression through ODE solver based on x0, t, and velocity_encoder\n",
    "    y0 = torch.tensor([1.0, 0, 1.0, 0, 1.0, 0] + torch.zeros(6).abs().tolist()).reshape(1, -1)\n",
    "    ys = torchsde.sdeint(sde, y0, t, method=\"euler\")\n",
    "\n",
    "    pre_u = ys[:, 0, :6]\n",
    "    pre_s = ys[:, 0, 6:]\n",
    "    pre_u = torch.clip(pre_u, 0)\n",
    "    pre_s = torch.clip(pre_s, 0)\n",
    "\n",
    "    pre_s = pd.DataFrame(pre_s.numpy())\n",
    "    pre_u = pd.DataFrame(pre_u.numpy())\n",
    "    gt_velo = np.array(pre_u) * beta - np.array(pre_s) * gamma\n",
    "    adata = AnnData(np.array(pre_s))\n",
    "\n",
    "    ## Preprocessing\n",
    "    adata.layers[\"Ms\"] = np.array(pre_s)\n",
    "    adata.layers[\"Mu\"] = np.array(pre_u)\n",
    "    adata.layers[\"spliced\"] = np.array(pre_s)\n",
    "    adata.layers[\"unspliced\"] = np.array(pre_u)\n",
    "    adata.obs_names = [f\"Cell_{i:d}\" for i in range(adata.n_obs)]\n",
    "    adata.var_names = [f\"Gene_{i:d}\" for i in range(adata.n_vars)]\n",
    "\n",
    "    adata.obs[\"time\"] = t.numpy()\n",
    "    reg_bdata = adata.copy()\n",
    "    reg_bdata.uns[\"regulators\"] = adata.var.index.values\n",
    "    reg_bdata.uns[\"targets\"] = adata.var.index.values\n",
    "    reg_bdata.uns[\"skeleton\"] = np.ones((len(adata.var.index), len(adata.var.index)))\n",
    "    reg_bdata.uns[\"network\"] = np.ones((len(adata.var.index), len(adata.var.index)))\n",
    "\n",
    "    ## veloVI & RegVelo min-max scaling\n",
    "    reg_bdata = preprocess_data(reg_bdata, filter_on_r2=False)\n",
    "\n",
    "    ## delete self-regulation\n",
    "    W = reg_bdata.uns[\"skeleton\"].copy()\n",
    "    W = torch.tensor(np.array(W)).int()\n",
    "\n",
    "    REGVELOVI.setup_anndata(reg_bdata, spliced_layer=\"Ms\", unspliced_layer=\"Mu\")\n",
    "    reg_vae = REGVELOVI(reg_bdata, W=W, lam2=1, soft_constraint=False, simple_dynamics=True)\n",
    "    reg_vae.train()\n",
    "\n",
    "    adata_target = add_regvelo_outputs_to_adata(reg_bdata, reg_vae)\n",
    "    pre_t = adata_target.layers[\"latent_time_regvelo\"].mean(1)\n",
    "    pre_t = t.max() * ((pre_t - np.min(pre_t)) / (np.max(pre_t) - np.min(pre_t)))\n",
    "    adata_target.obs[\"latent_time_regvelo\"] = pre_t\n",
    "\n",
    "    ## print regvelo performance\n",
    "    corr_t_rgv.append(scipy.stats.spearmanr(pre_t, adata.obs[\"time\"])[0])\n",
    "    print(\"RegVelo: \" + str(scipy.stats.spearmanr(pre_t, adata.obs[\"time\"])[0]))\n",
    "\n",
    "    ## compare velocity correlation\n",
    "    pre_velo = np.array(pre_u) * np.array(adata_target.var[\"fit_beta_regvelo\"]) - np.array(pre_s) * np.array(\n",
    "        adata_target.var[\"fit_gamma_regvelo\"]\n",
    "    )\n",
    "    corr_rgv = []\n",
    "    for i in range(6):\n",
    "        corr_rgv.append(scipy.stats.pearsonr(pre_velo[:, i], gt_velo[:, i])[0])\n",
    "\n",
    "    ## print regvelo performance\n",
    "    velo_rgv.append(np.mean(corr_rgv))\n",
    "    print(\"RegVelo: \" + str(np.mean(corr_rgv)))\n",
    "\n",
    "    ## Run scVelo\n",
    "    sc.pp.neighbors(reg_bdata)\n",
    "    scv.tl.recover_dynamics(reg_bdata, fit_scaling=False, var_names=adata.var_names, n_jobs=1)\n",
    "    reg_bdata.var[\"fit_scaling\"] = 1.0\n",
    "    scv.tl.velocity(reg_bdata, mode=\"dynamical\", min_likelihood=-np.inf, min_r2=None)\n",
    "    pre_t = reg_bdata.layers[\"fit_t\"].mean(1)\n",
    "    pre_t = t.max() * ((pre_t - np.min(pre_t)) / (np.max(pre_t) - np.min(pre_t)))\n",
    "\n",
    "    ## print regvelo performance\n",
    "    corr_t_scv.append(scipy.stats.spearmanr(pre_t, adata.obs[\"time\"])[0])\n",
    "    print(\"scVelo: \" + str(scipy.stats.spearmanr(pre_t, adata.obs[\"time\"])[0]))\n",
    "\n",
    "    ## compare velocity correlation\n",
    "    pre_velo = np.array(pre_u) * np.array(reg_bdata.var[\"fit_beta\"]) - np.array(pre_s) * np.array(\n",
    "        reg_bdata.var[\"fit_gamma\"]\n",
    "    )\n",
    "    corr_scv = []\n",
    "    for i in range(6):\n",
    "        corr_scv.append(scipy.stats.pearsonr(pre_velo[:, i], gt_velo[:, i])[0])\n",
    "\n",
    "    ## print regvelo performance\n",
    "    velo_scv.append(np.mean(corr_scv))\n",
    "    print(\"scVelo: \" + str(np.mean(corr_scv)))\n",
    "\n",
    "    ## import veloVI\n",
    "    VELOVI.setup_anndata(adata_target, spliced_layer=\"Ms\", unspliced_layer=\"Mu\")\n",
    "    vae = VELOVI(adata_target)\n",
    "    vae.train()\n",
    "    add_velovi_outputs_to_adata(adata_target, vae)\n",
    "    pre_t = adata_target.layers[\"fit_t\"].mean(1)\n",
    "    pre_t = 200 * ((pre_t - np.min(pre_t)) / (np.max(pre_t) - np.min(pre_t)))\n",
    "\n",
    "    corr_t_velovi.append(scipy.stats.spearmanr(pre_t, adata.obs[\"time\"])[0])\n",
    "    print(\"veloVI: \" + str(scipy.stats.spearmanr(pre_t, adata.obs[\"time\"])))\n",
    "\n",
    "    ## compare velocity\n",
    "    pre_velo = np.array(pre_u) * np.array(adata_target.var[\"fit_beta_velovi\"]) - np.array(pre_s) * np.array(\n",
    "        adata_target.var[\"fit_gamma_velovi\"]\n",
    "    )\n",
    "    corr = []\n",
    "    for i in range(6):\n",
    "        corr.append(scipy.stats.pearsonr(pre_velo[:, i], gt_velo[:, i])[0])\n",
    "\n",
    "    ## print regvelo performance\n",
    "    velo_velovi.append(np.mean(corr))\n",
    "    print(\"veloVI: \" + str(np.mean(corr)))\n",
    "\n",
    "    adata_target.obs[\"latent_time_velovi\"] = pre_t\n",
    "    ### calculate diffusion pseudotime\n",
    "    adata_target.uns[\"iroot\"] = np.flatnonzero(adata_target.obs[\"time\"] == 0)[0]\n",
    "\n",
    "    sc.pp.neighbors(adata_target)\n",
    "    sc.tl.diffmap(adata_target)\n",
    "    sc.tl.dpt(adata_target)\n",
    "\n",
    "    adata_target.obs[\"emperical_time\"] = adata_target.layers[\"Mu\"].mean(1)\n",
    "    corr_t_emperical.append(scipy.stats.spearmanr(adata_target.obs[\"emperical_time\"], adata.obs[\"time\"])[0])\n",
    "    corr_t_dpt.append(scipy.stats.spearmanr(adata_target.obs[\"dpt_pseudotime\"], adata.obs[\"time\"])[0])\n",
    "\n",
    "    #### Benchmark GRN performance\n",
    "    GRN = (\n",
    "        reg_vae.module.v_encoder.GRN_Jacobian(torch.tensor(np.array(pre_s)).mean(0).to(\"cuda:0\"))\n",
    "        .cpu()\n",
    "        .detach()\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    pre = GRN[np.where(~np.eye(GRN.shape[0], dtype=bool))]\n",
    "    label = K[np.where(~np.eye(K.shape[0], dtype=bool))]\n",
    "    label[label != 0] = 1\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(label, pre, pos_label=1)\n",
    "    rgv_GRN.append(sklearn.metrics.auc(fpr, tpr))\n",
    "\n",
    "    # calculate correlation\n",
    "    C = np.abs(np.array(pd.DataFrame(adata.layers[\"spliced\"]).corr()))\n",
    "    pre2 = C[np.where(~np.eye(C.shape[0], dtype=bool))]\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(label, pre2, pos_label=1)\n",
    "    cor_GRN.append(sklearn.metrics.auc(fpr, tpr))\n",
    "\n",
    "    # GRNBoost2\n",
    "    ex_matrix = adata.to_df(\"spliced\")\n",
    "    tf = ex_matrix.columns.tolist()\n",
    "    network = grnboost2(expression_data=ex_matrix, tf_names=tf)\n",
    "    table = np.array(network)\n",
    "\n",
    "    # Get unique TFs and targets\n",
    "    unique_tfs = np.unique(table[:, 0])\n",
    "    unique_targets = np.unique(table[:, 1])\n",
    "\n",
    "    # Create a new NumPy array to store the rearranged data\n",
    "    GRN = np.zeros((len(unique_targets), len(unique_tfs)))\n",
    "\n",
    "    # Fill in the new array with importance values\n",
    "    for row in table:\n",
    "        tf_index = np.where(unique_tfs == row[0])[0][0]\n",
    "        target_index = np.where(unique_targets == row[1])[0][0]\n",
    "        GRN[target_index, tf_index] = row[2]\n",
    "\n",
    "    pre = GRN[np.where(~np.eye(GRN.shape[0], dtype=bool))]\n",
    "    label = K[np.where(~np.eye(K.shape[0], dtype=bool))]\n",
    "    label[label != 0] = 1\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(label, pre, pos_label=1)\n",
    "    gb_GRN.append(sklearn.metrics.auc(fpr, tpr))\n",
    "\n",
    "    ## Run celloracle\n",
    "    base_GRN_sim = np.array(W)\n",
    "    base_GRN_sim[base_GRN_sim != 0] = 1\n",
    "    base_GRN_sim = pd.DataFrame(base_GRN_sim, columns=[\"Gene0\", \"Gene1\", \"Gene2\", \"Gene3\", \"Gene4\", \"Gene5\"])\n",
    "    base_GRN_sim.loc[:, \"peak_id\"] = [(f\"Peak_{i}\") for i in [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"]].copy()\n",
    "    base_GRN_sim.loc[:, \"gene_short_name\"] = [\"Gene0\", \"Gene1\", \"Gene2\", \"Gene3\", \"Gene4\", \"Gene5\"]\n",
    "    base_GRN_sim = base_GRN_sim.loc[\n",
    "        :, [\"peak_id\", \"gene_short_name\", \"Gene0\", \"Gene1\", \"Gene2\", \"Gene3\", \"Gene4\", \"Gene5\"]\n",
    "    ]\n",
    "\n",
    "    adata.var.index = [\"Gene0\", \"Gene1\", \"Gene2\", \"Gene3\", \"Gene4\", \"Gene5\"]\n",
    "    net = co.Net(\n",
    "        gene_expression_matrix=adata.to_df(),  # Input gene expression matrix as data frame\n",
    "        TFinfo_matrix=base_GRN_sim,  # Input base GRN\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    net.fit_All_genes(bagging_number=100, alpha=1, verbose=True)\n",
    "    net.updateLinkList(verbose=True)\n",
    "    inference_result = net.linkList.copy()\n",
    "\n",
    "    GRN_table = inference_result.iloc[:, :3].copy()\n",
    "    table = np.array(GRN_table)\n",
    "\n",
    "    # Get unique TFs and targets\n",
    "    unique_tfs = np.unique(table[:, 0])\n",
    "    unique_targets = np.unique(table[:, 1])\n",
    "\n",
    "    # Create a new NumPy array to store the rearranged data\n",
    "    GRN = np.zeros((len(unique_targets), len(unique_tfs)))\n",
    "\n",
    "    # Fill in the new array with importance values\n",
    "    for row in table:\n",
    "        tf_index = np.where(unique_tfs == row[0])[0][0]\n",
    "        target_index = np.where(unique_targets == row[1])[0][0]\n",
    "        GRN[target_index, tf_index] = row[2]\n",
    "    pre = np.abs(GRN)[np.where(~np.eye(GRN.shape[0], dtype=bool))]\n",
    "    label = K[np.where(~np.eye(K.shape[0], dtype=bool))]\n",
    "    label[label != 0] = 1\n",
    "    fpr, tpr, thresholds = sklearn.metrics.roc_curve(label, pre, pos_label=1)\n",
    "    co_GRN.append(sklearn.metrics.auc(fpr, tpr))\n",
    "\n",
    "    print(\n",
    "        \"AUC: \"\n",
    "        + \"RegVelo: \"\n",
    "        + str(rgv_GRN[len(rgv_GRN) - 1])\n",
    "        + \" Cor: \"\n",
    "        + str(cor_GRN[len(cor_GRN) - 1])\n",
    "        + \" GRNBoost2: \"\n",
    "        + str(gb_GRN[len(gb_GRN) - 1])\n",
    "        + \" CellOracle: \"\n",
    "        + str(co_GRN[len(co_GRN) - 1])\n",
    "    )\n",
    "    print(\n",
    "        \"Velocity: \"\n",
    "        + \"RegVelo: \"\n",
    "        + str(velo_rgv[len(velo_rgv) - 1])\n",
    "        + \" veloVI: \"\n",
    "        + str(velo_velovi[len(velo_velovi) - 1])\n",
    "        + \" scVelo: \"\n",
    "        + str(velo_scv[len(velo_scv) - 1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ccd7d-48d6-409a-93e8-0ff8a190178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rgv_GRN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9261771-dee7-440c-afff-48518505d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.DataFrame(\n",
    "    {\n",
    "        \"GRN\": rgv_GRN + cor_GRN + gb_GRN + co_GRN,\n",
    "        \"Model\": [\"RegVelo\"] * 100 + [\"Correlation\"] * 100 + [\"GRNBoost2\"] * 100 + [\"CellOracle\"] * 100,\n",
    "    }\n",
    ")\n",
    "if SAVE_DATASETS:\n",
    "    dat.to_csv(DATA_DIR / \"simulation\" / \"toy_GRN\" / \"GRN_benchmark_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c57ef672-b843-4c23-99d5-4c7ea51238a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## boxplot to show latent time correlation on each gene\n",
    "dat = pd.DataFrame(\n",
    "    {\n",
    "        \"Time\": corr_t_rgv + corr_t_velovi + corr_t_scv + corr_t_dpt,\n",
    "        \"Model\": [\"RegVelo\"] * 100 + [\"veloVI\"] * 100 + [\"scVelo\"] * 100 + [\"Diffusion pseudotime\"] * 100,\n",
    "    }\n",
    ")\n",
    "if SAVE_DATASETS:\n",
    "    dat.to_csv(DATA_DIR / \"simulation\" / \"toy_GRN\" / \"latent_time_benchmark_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7677d0ad-c0f4-4f10-bb98-13a865d869bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.DataFrame({\"RegVelo\": velo_rgv, \"scVelo\": velo_scv, \"veloVI\": velo_velovi})\n",
    "if SAVE_DATASETS:\n",
    "    dat.to_csv(DATA_DIR / \"simulation\" / \"toy_GRN\" / \"velocity_benchmark.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:celloracle_env]",
   "language": "python",
   "name": "conda-env-celloracle_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
